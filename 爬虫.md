```python
robots.txt协议：
    君子协议，规定哪些可爬取哪些不可爬取

http协议：超文本传输协议
    （服务器和客户端的数据交互一种形式）
    常用请求头信息：
        - User-Agent：请求载体的身份信息（Google/Baidu等）
        - Connection：请求完毕后，是否断开连接或保持连接
    常用响应头信息：
        - Content-Type：服务器响应回客户端的数据类型

https协议：安全的http协议--->传输交互时数据加密
    数据加密方式：
        - 对称密钥加密：客户端加密---密文和密钥发送给服务端---服务器接收并解密
            （*密文密钥同时发送，被拦截时直接可破解信息）
        - 非对称密钥加密：服务端加密---公开密钥发送给客户端---客户端用公钥对数据加密---
                       数据发送给服务端---服务端私有密钥解密
            （*效率低    *公钥被拦截篡改，再发送给客户端获取数据）
        - 证书密钥加密（https）：服务器加密---公开密钥发送给【证书认证机构】---
                    对公钥进行数字签名（防伪）---公钥封装在证书中发送给服务器     
```



```python
网络请求模块：urllib模块（古老且效率低）            requests模块
requests模块：---->模拟浏览器发送请求
    *功能强大    *简单便捷    *效率高
requests.status_code返回状态，成功访问返回值为200

使用：（requests模块的编码流程）
    - 指定url（网址）：
            step1：import requests（调取requests库）
            step2：url1='____'(指定url，以字符串的方式对网址进行封装）
    - 发起请求（get/poss）
            step3：response=requests.get(url=url1)（对该网址进行get请求，返回一个响应对象）
    - 获取响应数据
            step4：page_text=response.text（返回的字符串为响应数据的源码）
    - 持久化存储
            step5：with open('./___','w',encoding='UTF-8') as fp:（以写的方式（'w'）打开名为某                                                某的文件，并以某编码形式赋给as后的变量标识符）
                          fp.write(page_text)（将page_text的内容写进fp中）
注：./表示目前所在目录    ../表示上一层目录（多个表示上n层目录）        /表示根目录
```

```python
反爬机制（一）：
    UA检测（User-Agent）：服务器检测对应请求的载体身份标识
                        如果身份信息为某一浏览器----->正常用户请求----->允许获取数据
                        载体身份信息不是浏览器----->爬虫访问请求----->拒绝获取数据
反反爬策略：
    UA伪装：让爬虫身份表示伪装成某一浏览器，将User-Agent封装到一个字典中
            head={'User-Agent':'_______'}        后面字符串为检测工具中某一浏览器身份信息
            response=requests.get(url=url,params=param,headers=head)
注：requests.get()三个参数：1.url=    所请求的网址
                         2.params= 所请求网址所携带的参数（搜索等）--- 以'query'为键的字典封装
    （反反爬策略）            3.headers= 爬虫程序借用浏览器身份信息--- 以'User-Agent'为键的字典封装
```



```python
破解百度翻译：
    - 在输入栏输入时页面局部刷新：ajax请求 ---> 检测抓包工具
            * 在网络中查找sug包（post访问类型，负载中kw参数为所搜索内容）
            * 查看响应头Content-Typ，数据类型为json ---> 全称为JavaScript Object Notation
    - 响应头Content-Type为json，返回数据为json类型
    - requests.post中，参数由params变为data，获取响应数据为json型
            responce.json()        responce.text    text无参数表
    - 持久化存储：
        存储所得到的json文件：
            - fp = open('./dog.json','w',encoding='utf-8')
            - json.dump(responce.json(),fp=fp,ensure_ascii=False)（含有中文不能使用ASCII码）
    注：*open也可以用with open的格式打开文件
       *不能用之前的fp.write()直接写入fp文件，json文件要用json.dump()进行录入
       *json.dump要导入import json ---> json.dump(obj，fp=，*，ensure_ascii= )
```

```python
百度翻译Ⅱ：（被拦截，未处理）
    - 在检查工具XHR中找到响应地址https://fanyi.baidu.com/v2transapi?from=zh&to=en
    - post类型访问，ajson响应数据类型，找到user-agent，cookie，referer三个伪装数据
                封装在一个字典中，当作一个参数传入header
    - 负载中参数有：from，to（from表示原语言，to表示目标语言）
                  query（文本信息）
                  sign（查找sign生成函数并以js类型打开）
      以及以下无关常量参数：【所有参数封装在一个字典里，当作一个参数传入】
                  transtype: realtime
                     simple_means_flag: 3
                  token: 25e1bf3ced50e0245c4dc4762c94de9b
                  domain: common
    - 下载PyExecjs，调用execjs库【以下为生成sign函数的js文件，在python中调用js内容】
        def exec_js(name):
            with open(file=r'C:\AhaCpp\resources\Python编译\venv\爬虫程序\百度翻译搜索地址\sign生成函数.js', encoding="utf-8") as f:
                js_file = f.read()
            com = execjs.compile(js_file)  # 编译js文件
            results = com.call("e", name)  # 调用js里面的方法
            return results
```



```python
os包: os.remove(dir) -----> 删除dir文件，若不存在dir文件则报错
      os.path.exists(dir) -----> 检验dir文件是否存在，返回Ture or False
      os.mkdir(dir) -----> 创建dir文件夹
注: dir为文件路径，前缀为./或/

通过抓包工具响应数据，搜索判断该url所获取的信息是否为需要获取信息（判断静态加载，还是ajax动态加载）
    找寻所需爬取的数据：post/get    json/html    ajax/Not    params/data
```

```python
数据解析：----> 聚焦式爬虫：爬取页面中指定的页面内容
解析原理：
    从通用爬虫获取的html中找寻所需内容存储的标签，或标签内的属性
    step1：进行指定标签定位
    step2：对标签或标签属性中储存值进行提取
聚焦爬虫：
    step1：指定url
    step2：发起请求
    step3：获取相应数据
    step4：数据解析
    step5：持久化存储
三大内容（正则，bs4，xpath）：
    - 正则表达式：
    - bs4（BeautifulSoup）：
    - xpath（通用性强）：
```

```python
正则之爬取图片：
    【.content - 二进制，以.jpg文件打开】
    【.json() - 对象类型，以.js文件打开】
	【.text/.get_text() - 字符串，以.html文件打开或返回标签下所有文本内容】
    【.string - 字符串，返回直系标签的文本内容】
    
单张版：
	img_data = requests.get(url).content（content将响应数据转化为二进制形式【图片】）
  整页版：
	step1：获取整个页面数据（通用爬虫步骤step1.2.3）
    step2：对图片进行数据解析：
    	- 在网页html中找到储存所有图片的总div标签
        - 通过正则表达式，提取图片所存储的标签或属性值，重复性提取
        - 以os模块创建文件夹进行存储
        		import os
            	if not os.path.exists('文件名'):
                    os.mkdir('文件名')
          以循环的方式将图片进行存储进该文件路径（以二进制形式，存储mode需要用'wb'）
        - 页面处理 - 建立通用url - format循环格式化处理url
```

```python
bs4:(python独有)
	step1:
        - 实例化一个BeautifulSoup对象，并将源码数据加载到该数据中
    step2:
        - 标签定位/提取数据: 调用BeautifulSoup对象中【相关属性或方法】进行定位和提取
    step3:
        - 提取html标签下内容: 需要.text【标签下所有文本内容】或.string【标签下直系文本内容】转化为内部所需要的文本数据
        - 提取html标签内属性: soup.tagName['属性名称']，不需要转化数据

实例化BeautifulSoup		【实例化：用类（class）创建对象】
		- 导包：import requests
    		   from bs4 import BeautifulSoup
    	- 对象的实例化：( 1.将本地的html文档数据加载到该对象中
        					fp = open('text.html','r',encoding='utf-8')#文件描述符
            				soup = BeautifulSoup(fp,'lxml')#该对象实例化完成 )
        			   2.将往上获取的页面源码加载到该对象中
                    		page_text = requests.post/get(url,data,headers).text
                        	soup = BeautifulSoup(page_text,'lxml') #用lxml实例化一个对象
        - 分析获取源码，找寻所需要的标签地址或标签属性
        - 用BeautifulSoup的方法属性定位提取：
               * 1.soup.tagName
               * 2.soup.find('tagName'，class_='属性值') 
               * 3.soup.find_all('tagName')
                 4.soup.select('某种选择器（id，class，标签）')
                  
BeautifulSoup的方法和属性：
	- soup.tagName : tagName为soup中的某一个标签名，返回html内第一个tagName标签内容(包括标签)
    - soup.find('tagName'，class_='属性值') 
    		1:无属性值传入时，默认属性值为第一个标签，同上
        	2:有属性值时，属性值为标签内相应的属性值#注意class下划线
    - soup.find_all('tagName') : 返回【所有】该tagName标签值（同re.findall()，见正则笔记）
    - soup.select('某种选择器（id，class，标签）')
    			以列表形式返回所查找的标签 #不支持windows系统
    
注：.tagName/find只可返回一个标签或标签属性，.find_all/.select可返回多个标签内容
```



```python
xpath:
    step1:
        - 实例化一个etree对象，并将源码数据加载到该数据中
	step2:
        - 标签定位/提取数据: 调用etree对象中【方法和xpath表达式】进行定位和提取

xpath实例化：
	- 导包: from lxml import etree
           import requests
      page_text = requests.get/post(url,data,headers).text
    - 实例化对象: 1.实例化本地html文档
    			    tree = etree.parse(filePath) #filePath为文本路径
      			2.实例化获取到的数据源码
            		page_text = requests.get/post(url,data,headers).text
    				tree = etree.HTML(page_text)
    - 调xpath的方法: r = tree.xpath('xpath表达式')
    
xpath表达式：   		【类似于selec层级表达式】
# 查找div标签三种方式
	r = tree.xpath('/html/body/div')   # '/'表示从根目录开始定位，层层定位
    r = tree.xpath('/html//div')	   # 1.'//'表示多个层级(中间有跳过中间层级)
    r = tree.xpath('//div')			   # 2.'//'表示任意查找
# 通过属性定位标签
	r = tree.xpath('//div[@class="tang"]') 		     # [ ]内，@为属性标识
    r = tree.xpath('//div[@class="tang"]//li[5]')    # xpath的下标不同列表，是【从1开始】
# element对象取文本 ---> 【/text()和//text()】
    r = tree.xpath('//li[7]/a/text()')	  # /text()，标签内【直系】element对象转化为文本内容
    r = tree.xpath('//li[7]//text()')	  # //text()，标签内【所有】element对象转化为文本内容
# element对象取属性 ---> 【/@attrName】
	r = tree.xpath('//div[@class="song"]/img/@src')	  # img标签内的src属性值
    
注: xpath表达式可以用'|'或运算符进行两个将进行统一操作的两个不同表达式的一次性定位
```

```python
xpath之58同城：
 # 通过xpath表达式定位有多种路径，不一定表达式相同
	page_text = requests.get(url,headers).text
    tree = etree.HTML(page_text)
    li_list = tree.xpath('_______') # 存在多个<li>标签
    fp = open('58.txt','w',encoding = 'utf-8')
    for li in li_list:
        # 局部解析
        title = li.xpath('./_____/text') # './' 表示从当前li标签开始定位
        fp.write(title+'\n')
    fp.close()
    
中文乱码问题：
	- 修改响应数据编码：
    		response.encoding = 'gbk' # 或者utf-8
    - 修改局部乱码数据：【通用】
    		(name为乱码时)
        	name = name.encode(iso-8859-1).decode('gbk')
```

